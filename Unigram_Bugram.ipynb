{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gepQfUS2cuhT"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NY3VRXgeq4n9"
      },
      "outputs": [],
      "source": [
        "train_data_AD = './Data/train_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HYEfLNEQ1QCL"
      },
      "outputs": [],
      "source": [
        "train_data = []\n",
        "with open(train_data_AD, 'r') as f:\n",
        "    train_data = [line.rstrip().split(',') for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NY3VRXgeq4n9"
      },
      "outputs": [],
      "source": [
        "test_data_AD = './Data/test_data.csv'\n",
        "valid_data_AD = './Data/valid_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HYEfLNEQ1QCL"
      },
      "outputs": [],
      "source": [
        "test_data = []\n",
        "with open(test_data_AD, 'r') as f:\n",
        "    test_data = [line.rstrip().split(',') for line in f]\n",
        "\n",
        "valid_data = []\n",
        "with open(valid_data_AD, 'r') as f:\n",
        "    valid_data = [line.rstrip().split(',') for line in f]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TF(document) :\n",
        "    tokens = [list(gensim.utils.tokenize(doc, lower=True)) for doc in document]\n",
        "    CUSTOM_FILTERS = [remove_stopwords]\n",
        "    tokens = [preprocess_string(\" \".join(doc), CUSTOM_FILTERS) for doc in tokens]\n",
        "\n",
        "    for words in tokens :\n",
        "        word_count = {}\n",
        "        for word in words:\n",
        "            if word not in word_count:\n",
        "                word_count[word] = 1\n",
        "            elif word in word_count:\n",
        "                word_count[word] += 1\n",
        "    \n",
        "    return word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def CF(word_count, occurence) :\n",
        "    if occurence in word_count.keys() :\n",
        "        return word_count[occurence]\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def C(word_count, occurences) :\n",
        "    c = 0\n",
        "    for occurence in occurences :\n",
        "        c += CF(word_count, occurence)\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_same_words(test_word, train_word) :\n",
        "    test_index = []\n",
        "    train_index= []\n",
        "\n",
        "    for w in test_word :\n",
        "        if w in train_word :\n",
        "            test_index.append(test_word.index(w))\n",
        "            train_index.append(train_word.index(w))\n",
        "\n",
        "    return [test_index, train_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(x, y):\n",
        "    if len(x) != len(y) :\n",
        "        return None\n",
        "\n",
        "    cosine_similarity = dot(x, y)/(norm(x)*norm(y))\n",
        "    \n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_TF_IDF_Vector(document) :\n",
        "    tokens = [list(gensim.utils.tokenize(doc, lower=True)) for doc in document]\n",
        "    CUSTOM_FILTERS = [remove_stopwords]\n",
        "    tokens = [preprocess_string(\" \".join(doc), CUSTOM_FILTERS) for doc in tokens]\n",
        "\n",
        "    listToStr = []\n",
        "    for s in tokens :\n",
        "        listToStr.append(' '.join(map(str, s)))\n",
        "\n",
        "    g_dict = corpora.Dictionary([simple_preprocess(line) for line in listToStr])\n",
        "    g_bow = [g_dict.doc2bow(simple_preprocess(line)) for line in listToStr]\n",
        "\n",
        "    g_tfidf = gensim.models.TfidfModel(g_bow, smartirs='ntc')\n",
        "\n",
        "    return [g_dict, g_bow, g_tfidf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_words_with_tf_idf(document) :\n",
        "    g_dict, g_bow, g_tfidf = calculate_TF_IDF_Vector([document])\n",
        "\n",
        "    tf_idf = []\n",
        "    words = []\n",
        "    for item in g_tfidf[g_bow]:\n",
        "        for id, freq in item :\n",
        "            words.append(g_dict[id])\n",
        "            tf_idf.append(np.around(freq, decimals=2))\n",
        "\n",
        "    return [words, tf_idf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MAP(precision) :\n",
        "    if len(precision) == 0 :\n",
        "        return 0\n",
        "    \n",
        "    s = 0\n",
        "    for pre in precision :\n",
        "        if len(pre) > 0 :\n",
        "            x = 0\n",
        "            for i in pre :\n",
        "                x += i\n",
        "            s += ( x / len(pre) )\n",
        "        \n",
        "    return s / len(precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### P@k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Precision5(precision) :\n",
        "    if len(precision) < 5 :\n",
        "        return 0\n",
        "    return precision[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Precision10(precision) :\n",
        "    if len(precision) < 10 :\n",
        "        return 0\n",
        "    return precision[9]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MRR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MRR(reciprocal_rank) :\n",
        "    if len(reciprocal_rank) == 0 :\n",
        "        return 0\n",
        "    \n",
        "    s = 0\n",
        "    for rr in reciprocal_rank :\n",
        "        s += rr\n",
        "\n",
        "    return s / len(reciprocal_rank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assessment(test_data, train_data, qid1, qid2, vector) :\n",
        "    precision = []\n",
        "    reciprocal_rank = []\n",
        "    for i in range(len(vector)) :\n",
        "        maxs = np.sort(vector[i])[::-1]\n",
        "        maxs = maxs[0:10]\n",
        "    \n",
        "        pr = []\n",
        "        sum = 0\n",
        "        flag = True\n",
        "        for h in range(len(maxs)) :\n",
        "            index_max_similarity = np.where(vector[i] == maxs[h])[0][0]\n",
        "    \n",
        "            for k in range(len(test_data)) :\n",
        "                if test_data[k][1] == qid1[i] :\n",
        "                    test_s = test_data[k][-2]\n",
        "                    \n",
        "                    for z in range(len(train_data)) :\n",
        "                        if train_data[z][2] == qid2[index_max_similarity] :\n",
        "                            train_s = train_data[z][-2]\n",
        "                    \n",
        "                            if train_s == test_s :\n",
        "                                sum += 1\n",
        "                                pr.append(sum/(h + 1))\n",
        "                                if flag :\n",
        "                                    flag = False\n",
        "                                    reciprocal_rank.append(sum/(h + 1))\n",
        "                    \n",
        "        precision.append(pr)\n",
        "    \n",
        "    \n",
        "    for i in range(len(precision)) :\n",
        "        if len(precision[i]) > 0 :\n",
        "            print('Precision@5 for query ' + str(i) + ' = ' + str(Precision5(precision[i])) )\n",
        "            print('Precision@10 for query ' + str(i) + ' = ' + str(Precision10(precision[i])) )\n",
        "    \n",
        "    print('MAP = ' + str(MAP(precision)) )\n",
        "    print('MRR = ' + str(MRR(reciprocal_rank)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "qid2 = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    qid2.append(train_data[i][2])\n",
        "\n",
        "unique_res = np.unique(qid2[1:]) \n",
        "\n",
        "qid2 = list(unique_res) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['364931',\n",
              " '19408',\n",
              " '402550',\n",
              " '\"Mathematical Puzzles: What is () + () + () = 30 using 1',\n",
              " '3',\n",
              " '5',\n",
              " '7',\n",
              " '9',\n",
              " '11',\n",
              " '13',\n",
              " '15?\"',\n",
              " '\"How do I Simplify the following matrices:']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_data[i-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "train_data.pop(i)\n",
        "train_data.pop(i-1)\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unique Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16661\n"
          ]
        }
      ],
      "source": [
        "train_qs = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    train_qs.append(train_data[i][2])\n",
        "\n",
        "sentence = []\n",
        "for id in qid2 :\n",
        "    index = train_qs.index(id) + 1\n",
        "    sentence.append(train_data[index][-2])\n",
        "\n",
        "print(len(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16475"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indexes = [sentence.index(x) for x in set(sentence)]\n",
        "\n",
        "len(indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16475"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qid = []\n",
        "for i in indexes :\n",
        "    qid.append(qid2[i])\n",
        "\n",
        "qid2 = qid\n",
        "\n",
        "len(qid2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "qid1 = np.loadtxt(test_data_AD, delimiter = \",\", usecols = 1, dtype = str)\n",
        "\n",
        "unique_res = np.unique(qid1[1:]) \n",
        "\n",
        "qid1 = list(unique_res) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Valid questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "qid3 = np.loadtxt(valid_data_AD, delimiter = \",\", usecols = 1, dtype = str)\n",
        "\n",
        "unique_res = np.unique(qid3[1:]) \n",
        "\n",
        "qid3 = list(unique_res) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF √ó IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_51813/421210655.py:5: RuntimeWarning: invalid value encountered in divide\n",
            "  cosine_similarity = dot(x, y)/(norm(x)*norm(y))\n"
          ]
        }
      ],
      "source": [
        "test_qs = []\n",
        "for i in range(1, len(test_data)) :\n",
        "    test_qs.append(test_data[i][1])\n",
        "\n",
        "train_qs = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    train_qs.append(train_data[i][2])\n",
        "\n",
        "cos = []\n",
        "for q1 in qid1 :\n",
        "    index1 = test_qs.index(q1) + 1\n",
        "\n",
        "    test_words, test_tf_idf = match_words_with_tf_idf(test_data[index1][3])\n",
        "    \n",
        "    c = []\n",
        "    for q2 in qid2 :\n",
        "        index2 = train_qs.index(q2) + 1\n",
        "\n",
        "        train_words, train_tf_idf = match_words_with_tf_idf(train_data[index2][-2])\n",
        "        \n",
        "        test_index, train_index = find_same_words(test_words, train_words)\n",
        "\n",
        "        te = []\n",
        "        for index in test_index :\n",
        "            te.append(test_tf_idf[index])\n",
        "\n",
        "        tr = []\n",
        "        for index in train_index :\n",
        "            tr.append(train_tf_idf[index])\n",
        "\n",
        "        c.append(cosine_similarity(te, tr))\n",
        "        \n",
        "    cos.append(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assessment of Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(cos)) :\n",
        "    for j in range(len(cos[i])) :\n",
        "        if np.isnan(cos[i][j]) :\n",
        "            cos[i][j] = np.nan_to_num(cos[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@5 for query 0 = 1.6666666666666667\n",
            "Precision@10 for query 0 = 2.0\n",
            "Precision@5 for query 2 = 0\n",
            "Precision@10 for query 2 = 0\n",
            "Precision@5 for query 5 = 1.0\n",
            "Precision@10 for query 5 = 0\n",
            "Precision@5 for query 6 = 1.6666666666666667\n",
            "Precision@10 for query 6 = 2.0\n",
            "Precision@5 for query 7 = 0\n",
            "Precision@10 for query 7 = 0\n",
            "Precision@5 for query 11 = 1.0\n",
            "Precision@10 for query 11 = 0\n",
            "Precision@5 for query 13 = 0\n",
            "Precision@10 for query 13 = 0\n",
            "Precision@5 for query 15 = 0\n",
            "Precision@10 for query 15 = 0\n",
            "Precision@5 for query 17 = 0\n",
            "Precision@10 for query 17 = 0\n",
            "Precision@5 for query 21 = 0\n",
            "Precision@10 for query 21 = 0\n",
            "Precision@5 for query 26 = 0\n",
            "Precision@10 for query 26 = 0\n",
            "Precision@5 for query 29 = 0\n",
            "Precision@10 for query 29 = 0\n",
            "Precision@5 for query 32 = 0\n",
            "Precision@10 for query 32 = 0\n",
            "Precision@5 for query 33 = 0\n",
            "Precision@10 for query 33 = 0\n",
            "Precision@5 for query 34 = 0\n",
            "Precision@10 for query 34 = 0\n",
            "Precision@5 for query 46 = 0\n",
            "Precision@10 for query 46 = 0\n",
            "Precision@5 for query 53 = 0\n",
            "Precision@10 for query 53 = 0\n",
            "Precision@5 for query 64 = 2.5\n",
            "Precision@10 for query 64 = 2.5\n",
            "Precision@5 for query 65 = 5.0\n",
            "Precision@10 for query 65 = 10.0\n",
            "Precision@5 for query 66 = 5.0\n",
            "Precision@10 for query 66 = 10.0\n",
            "Precision@5 for query 71 = 2.5\n",
            "Precision@10 for query 71 = 2.5\n",
            "Precision@5 for query 73 = 0\n",
            "Precision@10 for query 73 = 0\n",
            "Precision@5 for query 75 = 0\n",
            "Precision@10 for query 75 = 0\n",
            "Precision@5 for query 76 = 0\n",
            "Precision@10 for query 76 = 0\n",
            "Precision@5 for query 80 = 0\n",
            "Precision@10 for query 80 = 0\n",
            "Precision@5 for query 81 = 0\n",
            "Precision@10 for query 81 = 0\n",
            "Precision@5 for query 87 = 0\n",
            "Precision@10 for query 87 = 0\n",
            "Precision@5 for query 92 = 1.0\n",
            "Precision@10 for query 92 = 1.0\n",
            "Precision@5 for query 93 = 5.0\n",
            "Precision@10 for query 93 = 10.0\n",
            "Precision@5 for query 108 = 2.5\n",
            "Precision@10 for query 108 = 3.3333333333333335\n",
            "Precision@5 for query 110 = 5.0\n",
            "Precision@10 for query 110 = 10.0\n",
            "Precision@5 for query 112 = 2.5\n",
            "Precision@10 for query 112 = 3.3333333333333335\n",
            "Precision@5 for query 113 = 5.0\n",
            "Precision@10 for query 113 = 5.0\n",
            "Precision@5 for query 116 = 0\n",
            "Precision@10 for query 116 = 0\n",
            "Precision@5 for query 122 = 0\n",
            "Precision@10 for query 122 = 0\n",
            "Precision@5 for query 124 = 0\n",
            "Precision@10 for query 124 = 0\n",
            "Precision@5 for query 126 = 5.0\n",
            "Precision@10 for query 126 = 10.0\n",
            "Precision@5 for query 127 = 5.0\n",
            "Precision@10 for query 127 = 0\n",
            "Precision@5 for query 129 = 2.5\n",
            "Precision@10 for query 129 = 3.3333333333333335\n",
            "Precision@5 for query 133 = 1.0\n",
            "Precision@10 for query 133 = 0\n",
            "Precision@5 for query 135 = 2.5\n",
            "Precision@10 for query 135 = 2.5\n",
            "Precision@5 for query 140 = 0\n",
            "Precision@10 for query 140 = 0\n",
            "MAP = 0.7555980482946603\n",
            "MRR = 1.0\n"
          ]
        }
      ],
      "source": [
        "assessment(test_data, train_data, qid1, qid2, cos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.savez('./Result/cosine_similarity.npz', cos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = np.load('./Result/cosine_similarity.npz')\n",
        "cos = data['arr_0']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "P(Q|D) = .* P(qi|D)\n",
        "\n",
        "Dirichlet :\n",
        "\n",
        "P(w|D) = (TFw,D + u CFw/|c|) / (|D| + u)\n",
        "\n",
        "|D| : length of D\n",
        "\n",
        "TFw,D : the number of occurrences of w in D\n",
        "\n",
        "CFw : the number of occurrences of w in the collection\n",
        "\n",
        "ùëê : œÉùë§ ùê∂ùêπùë§ : the total number of tokens in the collection\n",
        "\n",
        "landa = N / (N + u)\n",
        "\n",
        "1 - landa = u / (N + u)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate miu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_counts = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    word_counts.append(TF([train_data[i][-2]]))\n",
        "    \n",
        "d = {}\n",
        "for word_count in word_counts :\n",
        "    for word in word_count.keys() :\n",
        "        if word in d :\n",
        "            d[word] += word_count[word]\n",
        "        else :\n",
        "            d[word] = word_count[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_pwd = 0\n",
        "best_u = 0\n",
        "for v in np.arange(0.1, 2, 0.1) :\n",
        "    u = np.around(v , decimals=2)\n",
        "\n",
        "    valid_qs = []\n",
        "    for i in range(1, len(valid_data)) :\n",
        "        valid_qs.append(valid_data[i][1])\n",
        "\n",
        "    train_qs = []\n",
        "    for i in range(1, len(train_data)) :\n",
        "        train_qs.append(train_data[i][2])\n",
        "\n",
        "    for q3 in qid3 :\n",
        "        index1 = valid_qs.index(q3) + 1\n",
        "\n",
        "        valid_word_count = TF([valid_data[index1][3]])\n",
        "\n",
        "        c = C(d, list(valid_word_count.keys()))\n",
        "\n",
        "        pw = []\n",
        "        for q2 in qid2 :\n",
        "            index2 = train_qs.index(q2) + 1\n",
        "\n",
        "            train_word_count = TF([train_data[index2][-2]])\n",
        "\n",
        "            p = 1\n",
        "            for valid_word in valid_word_count.keys() :\n",
        "                tf = 0\n",
        "                if valid_word in train_word_count.keys() :\n",
        "                    tf = train_word_count[valid_word]\n",
        "                p *= (tf + u * (CF(d, valid_word) / c) ) / (len(list(train_word_count.keys())) + u)\n",
        "\n",
        "            pw.append(p)\n",
        "\n",
        "        if max_pwd < max(pw) :\n",
        "            max_pwd = max(pw)\n",
        "            best_u = u\n",
        "\n",
        "print('Best mui : ' + str(best_u))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conditional probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_counts = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    word_counts.append(TF([train_data[i][-2]]))\n",
        "\n",
        "d = {}\n",
        "for word_count in word_counts :\n",
        "    for word in word_count.keys() :\n",
        "        if word in d :\n",
        "            d[word] += word_count[word]\n",
        "        else :\n",
        "            d[word] = word_count[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "u = 0.1\n",
        "\n",
        "test_qs = []\n",
        "for i in range(1, len(test_data)) :\n",
        "    test_qs.append(test_data[i][1])\n",
        "\n",
        "train_qs = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    train_qs.append(train_data[i][2])\n",
        "\n",
        "P_wd = []\n",
        "for q1 in qid1 :\n",
        "    index1 = test_qs.index(q1) + 1\n",
        "\n",
        "    test_word_count = TF([test_data[index1][3]])\n",
        "    \n",
        "    c = C(d, list(test_word_count.keys()))\n",
        "\n",
        "    pw = []\n",
        "    for q2 in qid2 :\n",
        "        index2 = train_qs.index(q2) + 1\n",
        "\n",
        "        train_word_count = TF([train_data[index2][-2]])\n",
        "        \n",
        "        p = 1\n",
        "        for test_word in test_word_count.keys() :\n",
        "            tf = 0\n",
        "            if test_word in train_word_count.keys() :\n",
        "                tf = train_word_count[test_word]\n",
        "            p *= (tf + u * (CF(train_word_count, test_word) / c) ) / (len(list(train_word_count.keys())) + u)\n",
        "        \n",
        "        pw.append(p)\n",
        "\n",
        "    P_wd.append(pw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assessment of Unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@5 for query 0 = 0.8333333333333334\n",
            "Precision@10 for query 0 = 0\n",
            "Precision@5 for query 2 = 0\n",
            "Precision@10 for query 2 = 0\n",
            "Precision@5 for query 15 = 0\n",
            "Precision@10 for query 15 = 0\n",
            "Precision@5 for query 17 = 1.25\n",
            "Precision@10 for query 17 = 1.6666666666666667\n",
            "Precision@5 for query 18 = 2.5\n",
            "Precision@10 for query 18 = 0\n",
            "Precision@5 for query 20 = 0\n",
            "Precision@10 for query 20 = 0\n",
            "Precision@5 for query 29 = 0\n",
            "Precision@10 for query 29 = 0\n",
            "Precision@5 for query 33 = 0\n",
            "Precision@10 for query 33 = 0\n",
            "Precision@5 for query 34 = 0\n",
            "Precision@10 for query 34 = 0\n",
            "Precision@5 for query 44 = 0\n",
            "Precision@10 for query 44 = 0\n",
            "Precision@5 for query 59 = 0\n",
            "Precision@10 for query 59 = 0\n",
            "Precision@5 for query 65 = 1.6666666666666667\n",
            "Precision@10 for query 65 = 2.0\n",
            "Precision@5 for query 76 = 0\n",
            "Precision@10 for query 76 = 0\n",
            "Precision@5 for query 79 = 0\n",
            "Precision@10 for query 79 = 0\n",
            "Precision@5 for query 84 = 0\n",
            "Precision@10 for query 84 = 0\n",
            "Precision@5 for query 85 = 0\n",
            "Precision@10 for query 85 = 0\n",
            "Precision@5 for query 88 = 0\n",
            "Precision@10 for query 88 = 0\n",
            "Precision@5 for query 89 = 2.5\n",
            "Precision@10 for query 89 = 2.5\n",
            "Precision@5 for query 90 = 0\n",
            "Precision@10 for query 90 = 0\n",
            "Precision@5 for query 96 = 5.0\n",
            "Precision@10 for query 96 = 10.0\n",
            "Precision@5 for query 97 = 2.5\n",
            "Precision@10 for query 97 = 0\n",
            "Precision@5 for query 101 = 0\n",
            "Precision@10 for query 101 = 0\n",
            "Precision@5 for query 104 = 1.6666666666666667\n",
            "Precision@10 for query 104 = 0\n",
            "Precision@5 for query 112 = 2.5\n",
            "Precision@10 for query 112 = 3.3333333333333335\n",
            "Precision@5 for query 113 = 5.0\n",
            "Precision@10 for query 113 = 5.0\n",
            "Precision@5 for query 116 = 0\n",
            "Precision@10 for query 116 = 0\n",
            "Precision@5 for query 121 = 2.5\n",
            "Precision@10 for query 121 = 2.5\n",
            "Precision@5 for query 122 = 0\n",
            "Precision@10 for query 122 = 0\n",
            "Precision@5 for query 124 = 0\n",
            "Precision@10 for query 124 = 0\n",
            "Precision@5 for query 128 = 0\n",
            "Precision@10 for query 128 = 0\n",
            "Precision@5 for query 133 = 1.0\n",
            "Precision@10 for query 133 = 0\n",
            "Precision@5 for query 142 = 0\n",
            "Precision@10 for query 142 = 0\n",
            "MAP = 0.3729019378095893\n",
            "MRR = 0.9557291666666666\n"
          ]
        }
      ],
      "source": [
        "assessment(test_data, train_data, qid1, qid2, P_wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.savez('./Result/PWD_unigram_01.npz', P_wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = np.load('./Result/PWD_unigram_01.npz')\n",
        "P_wd = data['arr_0']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "P(Q|D) = P(q1|D) .* P(qi|qi-1, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate Lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_counts = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    word_counts.append(TF([train_data[i][-2]]))\n",
        "\n",
        "d = {}\n",
        "for word_count in word_counts :\n",
        "    for word in word_count.keys() :\n",
        "        if word in d :\n",
        "            d[word] += word_count[word]\n",
        "        else :\n",
        "            d[word] = word_count[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_pwd = 0\n",
        "best_lamda = 0\n",
        "for v in np.arange(0.1, 2, 0.1) :\n",
        "    l = np.around(v , decimals=2)\n",
        "\n",
        "    u = 0.1\n",
        "\n",
        "    valid_qs = []\n",
        "    for i in range(1, len(valid_data)) :\n",
        "        valid_qs.append(valid_data[i][1])\n",
        "\n",
        "    train_qs = []\n",
        "    for i in range(1, len(train_data)) :\n",
        "        train_qs.append(train_data[i][2])\n",
        "\n",
        "    for q3 in qid3 :\n",
        "        index1 = valid_qs.index(q3) + 1\n",
        "\n",
        "        valid_word_count = TF([valid_data[index1][3]])\n",
        "\n",
        "        valid_keys = list(valid_word_count.keys())\n",
        "\n",
        "        c = C(d, valid_keys)\n",
        "\n",
        "        pw = []\n",
        "        for q2 in qid2 :\n",
        "            index2 = train_qs.index(q2) + 1\n",
        "\n",
        "            train_word_count = TF([train_data[index2][-2]])\n",
        "\n",
        "            if valid_keys[0] in train_word_count.keys() :\n",
        "                    tf = train_word_count[valid_keys[0]]\n",
        "            else : tf = 0\n",
        "\n",
        "            p = (tf + u * (CF(train_word_count, valid_keys[0]) / c) ) / (len(list(train_word_count.keys())) + u)\n",
        "\n",
        "            for i in range(2, len(valid_keys)) :\n",
        "                valid_word_j = valid_keys[i-1]\n",
        "                valid_word_i = valid_keys[i]\n",
        "                tf = 0\n",
        "                if valid_word_i in train_word_count.keys() :\n",
        "                    tf = train_word_count[valid_word_i]\n",
        "\n",
        "                if valid_word_j in train_word_count.keys() :\n",
        "                    unigram = (tf + u * (CF(train_word_count, valid_word_i) / c) ) / (len(list(train_word_count.keys())) + u)\n",
        "                    p *= (l * (C(train_word_count, [valid_word_j, valid_word_i]) / CF(train_word_count, valid_word_j))) + (l * unigram)\n",
        "\n",
        "            pw.append(p)\n",
        "\n",
        "        if max_pwd < max(pw) :\n",
        "                max_pwd = max(pw)\n",
        "                best_lamda = l\n",
        "\n",
        "print('Best landa : ' + str(best_lamda))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conditional probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_counts = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    word_counts.append(TF([train_data[i][-2]]))\n",
        "\n",
        "d = {}\n",
        "for word_count in word_counts :\n",
        "    for word in word_count.keys() :\n",
        "        if word in d :\n",
        "            d[word] += word_count[word]\n",
        "        else :\n",
        "            d[word] = word_count[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "l = 1.9\n",
        "u = 0.1\n",
        "\n",
        "test_qs = []\n",
        "for i in range(1, len(test_data)) :\n",
        "    test_qs.append(test_data[i][1])\n",
        "\n",
        "train_qs = []\n",
        "for i in range(1, len(train_data)) :\n",
        "    train_qs.append(train_data[i][2])\n",
        "\n",
        "P_wd = []\n",
        "for q1 in qid1 :\n",
        "    index1 = test_qs.index(q1) + 1\n",
        "\n",
        "    test_word_count = TF([test_data[index1][3]])\n",
        "        \n",
        "    test_keys = list(test_word_count.keys())\n",
        "\n",
        "    c = C(d, test_keys)\n",
        "\n",
        "    pw = []\n",
        "    for q2 in qid2 :\n",
        "        index2 = train_qs.index(q2) + 1\n",
        "\n",
        "        train_word_count = TF([train_data[index2][-2]])\n",
        "        \n",
        "        if test_keys[0] in train_word_count.keys() :\n",
        "                tf = train_word_count[test_keys[0]]\n",
        "        else : tf = 0\n",
        "        \n",
        "        p = (tf + u * (CF(train_word_count, test_keys[0]) / c) ) / (len(list(train_word_count.keys())) + u)\n",
        "\n",
        "        for i in range(2, len(test_keys)) :\n",
        "            test_word_j = test_keys[i-1]\n",
        "            test_word_i = test_keys[i]\n",
        "            tf = 0\n",
        "            if test_word_i in train_word_count.keys() :\n",
        "                tf = train_word_count[test_word_i]\n",
        "            \n",
        "            if test_word_j in train_word_count.keys() :\n",
        "                unigram = (tf + u * (CF(train_word_count, test_word_i) / c) ) / (len(list(train_word_count.keys())) + u)\n",
        "                p *= (l * (C(train_word_count, [test_word_j, test_word_i]) / CF(train_word_count, test_word_j))) + (l * unigram)\n",
        "        \n",
        "        pw.append(p)\n",
        "\n",
        "    P_wd.append(pw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assessment of Bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@5 for query 0 = 0.8333333333333334\n",
            "Precision@10 for query 0 = 0\n",
            "Precision@5 for query 3 = 0\n",
            "Precision@10 for query 3 = 0\n",
            "Precision@5 for query 4 = 0\n",
            "Precision@10 for query 4 = 0\n",
            "Precision@5 for query 6 = 1.6666666666666667\n",
            "Precision@10 for query 6 = 3.3333333333333335\n",
            "Precision@5 for query 8 = 0\n",
            "Precision@10 for query 8 = 0\n",
            "Precision@5 for query 9 = 2.5\n",
            "Precision@10 for query 9 = 2.5\n",
            "Precision@5 for query 18 = 2.5\n",
            "Precision@10 for query 18 = 0\n",
            "Precision@5 for query 20 = 0\n",
            "Precision@10 for query 20 = 0\n",
            "Precision@5 for query 21 = 0\n",
            "Precision@10 for query 21 = 0\n",
            "Precision@5 for query 24 = 2.5\n",
            "Precision@10 for query 24 = 3.3333333333333335\n",
            "Precision@5 for query 26 = 0\n",
            "Precision@10 for query 26 = 0\n",
            "Precision@5 for query 29 = 0\n",
            "Precision@10 for query 29 = 0\n",
            "Precision@5 for query 30 = 0\n",
            "Precision@10 for query 30 = 0\n",
            "Precision@5 for query 31 = 0\n",
            "Precision@10 for query 31 = 0\n",
            "Precision@5 for query 33 = 0\n",
            "Precision@10 for query 33 = 0\n",
            "Precision@5 for query 34 = 0\n",
            "Precision@10 for query 34 = 0\n",
            "Precision@5 for query 35 = 0\n",
            "Precision@10 for query 35 = 0\n",
            "Precision@5 for query 41 = 0\n",
            "Precision@10 for query 41 = 0\n",
            "Precision@5 for query 43 = 0\n",
            "Precision@10 for query 43 = 0\n",
            "Precision@5 for query 44 = 0\n",
            "Precision@10 for query 44 = 0\n",
            "Precision@5 for query 47 = 0\n",
            "Precision@10 for query 47 = 0\n",
            "Precision@5 for query 48 = 0\n",
            "Precision@10 for query 48 = 0\n",
            "Precision@5 for query 52 = 1.6666666666666667\n",
            "Precision@10 for query 52 = 0\n",
            "Precision@5 for query 53 = 0\n",
            "Precision@10 for query 53 = 0\n",
            "Precision@5 for query 54 = 0\n",
            "Precision@10 for query 54 = 0\n",
            "Precision@5 for query 55 = 1.0\n",
            "Precision@10 for query 55 = 1.25\n",
            "Precision@5 for query 58 = 1.25\n",
            "Precision@10 for query 58 = 0\n",
            "Precision@5 for query 59 = 0\n",
            "Precision@10 for query 59 = 0\n",
            "Precision@5 for query 64 = 0\n",
            "Precision@10 for query 64 = 0\n",
            "Precision@5 for query 65 = 1.6666666666666667\n",
            "Precision@10 for query 65 = 2.0\n",
            "Precision@5 for query 66 = 0\n",
            "Precision@10 for query 66 = 0\n",
            "Precision@5 for query 67 = 5.0\n",
            "Precision@10 for query 67 = 10.0\n",
            "Precision@5 for query 72 = 0.8333333333333334\n",
            "Precision@10 for query 72 = 1.6666666666666667\n",
            "Precision@5 for query 73 = 0\n",
            "Precision@10 for query 73 = 0\n",
            "Precision@5 for query 75 = 0\n",
            "Precision@10 for query 75 = 0\n",
            "Precision@5 for query 78 = 0\n",
            "Precision@10 for query 78 = 0\n",
            "Precision@5 for query 80 = 2.5\n",
            "Precision@10 for query 80 = 3.3333333333333335\n",
            "Precision@5 for query 83 = 2.5\n",
            "Precision@10 for query 83 = 1.0\n",
            "Precision@5 for query 84 = 1.6666666666666667\n",
            "Precision@10 for query 84 = 2.0\n",
            "Precision@5 for query 85 = 0\n",
            "Precision@10 for query 85 = 0\n",
            "Precision@5 for query 86 = 0\n",
            "Precision@10 for query 86 = 0\n",
            "Precision@5 for query 88 = 1.6666666666666667\n",
            "Precision@10 for query 88 = 1.25\n",
            "Precision@5 for query 89 = 2.5\n",
            "Precision@10 for query 89 = 2.5\n",
            "Precision@5 for query 91 = 0\n",
            "Precision@10 for query 91 = 0\n",
            "Precision@5 for query 96 = 5.0\n",
            "Precision@10 for query 96 = 10.0\n",
            "Precision@5 for query 97 = 2.5\n",
            "Precision@10 for query 97 = 0\n",
            "Precision@5 for query 101 = 1.6666666666666667\n",
            "Precision@10 for query 101 = 1.4285714285714286\n",
            "Precision@5 for query 104 = 1.6666666666666667\n",
            "Precision@10 for query 104 = 0\n",
            "Precision@5 for query 108 = 2.5\n",
            "Precision@10 for query 108 = 3.3333333333333335\n",
            "Precision@5 for query 112 = 2.5\n",
            "Precision@10 for query 112 = 3.3333333333333335\n",
            "Precision@5 for query 113 = 5.0\n",
            "Precision@10 for query 113 = 5.0\n",
            "Precision@5 for query 116 = 0\n",
            "Precision@10 for query 116 = 0\n",
            "Precision@5 for query 117 = 0.8333333333333334\n",
            "Precision@10 for query 117 = 0\n",
            "Precision@5 for query 118 = 5.0\n",
            "Precision@10 for query 118 = 10.0\n",
            "Precision@5 for query 120 = 0\n",
            "Precision@10 for query 120 = 0\n",
            "Precision@5 for query 121 = 2.5\n",
            "Precision@10 for query 121 = 2.5\n",
            "Precision@5 for query 122 = 0\n",
            "Precision@10 for query 122 = 0\n",
            "Precision@5 for query 128 = 0\n",
            "Precision@10 for query 128 = 0\n",
            "Precision@5 for query 129 = 2.5\n",
            "Precision@10 for query 129 = 3.3333333333333335\n",
            "Precision@5 for query 130 = 1.25\n",
            "Precision@10 for query 130 = 2.5\n",
            "Precision@5 for query 132 = 2.5\n",
            "Precision@10 for query 132 = 2.5\n",
            "Precision@5 for query 135 = 0\n",
            "Precision@10 for query 135 = 0\n",
            "Precision@5 for query 137 = 0\n",
            "Precision@10 for query 137 = 0\n",
            "Precision@5 for query 139 = 0\n",
            "Precision@10 for query 139 = 0\n",
            "Precision@5 for query 140 = 0\n",
            "Precision@10 for query 140 = 0\n",
            "Precision@5 for query 141 = 5.0\n",
            "Precision@10 for query 141 = 0\n",
            "Precision@5 for query 142 = 0\n",
            "Precision@10 for query 142 = 0\n",
            "MAP = 0.8971436142217515\n",
            "MRR = 0.8361288794124616\n"
          ]
        }
      ],
      "source": [
        "assessment(test_data, train_data, qid1, qid2, P_wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.savez('./Result/PWD_bigram_19_01.npz', P_wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = np.load('./Result/PWD_bigram_19_01.npz')\n",
        "P_wd = data['arr_0']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
